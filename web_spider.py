#!/home/peiran/anaconda3/bin/python

"""
Web spider to download and clean the html file from indeed
"""
############################
# Load modules
############################
from bs4 import BeautifulSoup # For HTML parsing
import urllib.request as ub
import re # Regular expressions
from time import sleep # To prevent overwhelming the server between connections
from collections import Counter # Keep track of our term counts
from nltk.corpus import stopwords # Filter out stopwords, such as 'the', 'or', 'and'
import numpy as np
import pandas as pd # For converting results to a dataframe and bar chart plots
import time 
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns
import sys


############################
# Job parser class
############################
class indeedDSParser():
    
    """
    This class is used to collect the location and skills requirement information of Data Science job on indeed
    """
    def __init__(self,n):
        self.info = []
        self.webStart = "http://www.indeed.com/jobs?q=data+science&jt=fulltime&fromage=1&start=0&pp="
        self.skillList = ['r','python','java','c++','ruby','perl','matlab','javascript','scala','excel','tableau',
                'd3.js','sas','d3','spss','hadoop','mapreduce','spark','pig','hive','zookeeper',
                'oozie','mahout','flume','sql','nosql','hbase','cassandra','mongodb']
        self.pageAmount = n
        self.linkSkill = []
    
    
    def text_extractor(self,website):
        '''Extract words from html file

	   	Input:
	   		website: url of the webpage of a job
	   	return:
	   		a python list containing words e.g.['available','coding','engineers',...]
	    '''
        try:
            site = ub.urlopen(website).read() 
        except: 
            return  
        
        # Get the html from the site
        soupObj = BeautifulSoup(site,"lxml") 

        for script in soupObj(["script", "style"]):
            script.extract() # Remove these two elements from the BS4 object

        text = soupObj.get_text() # Get the text from this
        
        # break into lines
        lines = (line.strip() for line in text.splitlines()) 
        # break multi-headlines into a line each
        chunks = (phrase.strip() for line in lines for phrase in line.split("  ")) 
        text = re.sub("[^a-zA-Z.+3]"," ", text)  
        # Go to lower case and split them apart
        text = text.lower().split()  
        # get distinct word counts
        text = list(set(text)) 
        return text
    
    
    def pageGenerator(self,n):
        """
        return the link of pages that will be used
        """
        links = []
        for i in range(n):
            front = "http://www.indeed.com/jobs?q=data+science&jt=fulltime&fromage=1&start="
            end = "&pp="
            links.append("".join([front,str(i*10),end]))
        return links

    def completeLink(self,link):
    	"""make the linke complete by adding "http://www.indeed.com"
		"""
    	return "http://www.indeed.com" + link

    def jobLinkCollector(self,pageAddr):
    	"""Collect all links to job webpages on each search page
	    Input:
	    	pageAddr: search page generated by pageGenerator()
	    return:
	    	a list of urls to each job webpage
	    """
        # load page
    	site = ub.urlopen(pageAddr).read()
    	soupObj = BeautifulSoup(site,"lxml")

		# find the division for the fixed 10 jobs on one page
    	jobs = soupObj.find_all("div",attrs = {"data-tn-component":"organicJob"})
    	jobLink = []

		# iteration for get the link of each job site
    	for i in range(len(jobs)):
    		link = jobs[i].find("a",attrs={"data-tn-element":"jobTitle"}).get("href")
    		jobLink.append(self.completeLink(link))

    	return jobLink
    
    
    def jobLocCollector(self,pageAddr):
    	"""Collect job locations information on each job webpage
    	"""
        # load page

    	site = ub.urlopen(pageAddr).read()
    	soupObj = BeautifulSoup(site,"lxml")

    	# find the division for the fixed 10 jobs on one page
    	jobs = soupObj.find_all("div",attrs = {"data-tn-component":"organicJob"})
    	jobLoc = []

    	# iteration for get the link of each job site
    	for i in range(len(jobs)):
    		loc = jobs[i].find("span",attrs={"itemprop":"addressLocality"}).contents[0]
    		loc = ''.join(i for i in loc if not i.isdigit()).strip()
    		jobLoc.append(loc)

    	return jobLoc
    
    def linkSkillInfo(self,n):
       
        # generate the pages for indeed jobs
        jobPages = self.pageGenerator(n)
        linkSkill= []
        
        for page in range(len(jobPages)):
        	print("##################################")
        	print('Collect job links on page {}'.format(page + 1))
        	links = self.jobLinkCollector(jobPages[page])
        	locs = self.jobLocCollector(jobPages[page])
        	info = [(x,y) for x,y in zip(links,locs)]
        	self.linkSkill.extend(info)
        print("##################################")
        print('Pages are have been generated')
    
    def locSkill(self,textList,skillList):
        """
        This function is used to get skill and loc info
        """ 
        for i in range(len(textList)):
            skillDict = dict((el,0) for el in skillList)
            for j in skillList:
                try:
                    skillDict[j] += int((np.array(textList[i])==j).sum())
                except:
                    skillDict[j] += 1 if np.array(textList[i])==j else 0
            skillDict["address"] = self.linkSkill[i][1]
            self.info.append(skillDict)

    def run(self):
        self.linkSkillInfo(self.pageAmount)
        infoList = []
        print("##################################")
        print("Start to download and cleanse webpages")
        for item in tqdm(self.linkSkill):
            words = self.text_extractor(item[0])
            infoList.append(words)
            time.sleep(0.5)
        
        self.locSkill(infoList,self.skillList)
    
    def get(self):
        return self.info

if __name__ == "__main__":

	n = int(sys.argv[1])

	# initialize the instance
	jobparser = indeedDSParser(n)

	# run the parser
	jobparser.run()

	# extract job information from jobparser
	jobData = jobparser.get()

	# transform to pandas frame and clean the data
	jobFrame = pd.DataFrame(jobData)
	jobFrame = jobFrame.ix[["," in x for x in jobFrame["address"]],:]
	jobFrame.rename(index=str, columns={"address": "state"},inplace=True)
	jobFrame['state'] = jobFrame['state'].apply(lambda x: x.split(",")[1])
	
	# save the data from to jobFrame.csv
	print("##################################")
	print("Job information saved to jobFrame.csv")
	jobFrame.to_csv('./data/jobFrame.csv',index=False)

	print("##################################")
	print("Part of jobFrame.csv")
	print(jobFrame.ix[:5,:8])

   












